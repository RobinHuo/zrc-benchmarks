{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "\"\"\" Testing sLM21 submission loading \"\"\"\n",
    "%env APP_DIR=/data/zerospeech/zrbench\n",
    "from pathlib import Path\n",
    "\n",
    "from zerospeech.benchmarks.sLM_21 import SLM21Submission\n",
    "\n",
    "submission1 = SLM21Submission.load(Path('/data/zerospeech/zrbench/samples/sLM21-random-submission'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: APP_DIR=/data/zerospeech/zrbench\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing abx-LS submission loading \"\"\"\n",
    "%env APP_DIR=/data/zerospeech/zrbench\n",
    "from pathlib import Path\n",
    "\n",
    "from zerospeech.benchmarks.abx_LS import AbxLSSubmission\n",
    "\n",
    "submission2 = AbxLSSubmission.load(Path('/data/zerospeech/zrbench/samples/abxLS-random-submission/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/nhamilakis/Seafile/workspace/coml/zerospeech/core/zerospeech-benchmarks/testing.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nhamilakis/Seafile/workspace/coml/zerospeech/core/zerospeech-benchmarks/testing.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m_file \u001b[39m=\u001b[39m submission2\u001b[39m.\u001b[39mmeta\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nhamilakis/Seafile/workspace/coml/zerospeech/core/zerospeech-benchmarks/testing.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m m_file\u001b[39m.\u001b[39;49mdir()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'dir'"
     ]
    }
   ],
   "source": [
    "m_file = submission2.meta\n",
    "m_file.dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test validation of sLM21 \"\"\"\n",
    "%env APP_DIR=/data/zerospeech/zrbench\n",
    "from zerospeech.benchmarks.sLM_21.validators import SLM21SubmissionValidator\n",
    "\n",
    "validator = SLM21SubmissionValidator()\n",
    "\n",
    "res = validator.validate(submission1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Test implementation of functions with internal state \"\"\"\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def length_comparison(dim: int):\n",
    "    data = dict(ncols=[], col_len=1)\n",
    "\n",
    "    def comparison(array: List[str]):\n",
    "        data['ncols'].append(len(array))\n",
    "        res = []\n",
    "\n",
    "        if len(set(data['ncols'])) != 1:\n",
    "            print(f\"found error {len(set(data['ncols']))}\")\n",
    "            res = ['error']\n",
    "        return res\n",
    "\n",
    "    return comparison\n",
    "\n",
    "\n",
    "test_1 = [[random.randint(1, 30) for _ in range(100)] for _ in range(50)]\n",
    "test_2 = [[random.randint(1, 30) for _ in range(100)] for _ in range(50)]\n",
    "\n",
    "# add two at random\n",
    "test_2[29].append(random.randint(1, 30))\n",
    "test_2[19].append(random.randint(1, 30))\n",
    "\n",
    "comp1 = length_comparison(2)\n",
    "comp2 = length_comparison(2)\n",
    "\n",
    "results1 = []\n",
    "results2 = []\n",
    "for i1, i2 in zip(test_1, test_2):\n",
    "    results1.extend(comp1(i1))\n",
    "    results2.extend(comp2(i2))\n",
    "\n",
    "print(results1)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Testing loading of scores dir of an sLM21 submission and exporting into leaderboard \"\"\"\n",
    "%env APP_DIR=/data/zerospeech/zrbench\n",
    "from zerospeech.benchmarks import sLM_21\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich import pretty\n",
    "pretty.install()\n",
    "\n",
    "\n",
    "con = Console()\n",
    "dataset = sLM_21.SLM21Dataset.load()\n",
    "\n",
    "dev_size = pd.read_csv(dataset.index.subsets.semantic_dev.items.pairs.file, header=0)\\\n",
    "    .groupby(['type', 'dataset'], as_index=False).size()\n",
    "test_size = pd.read_csv(dataset.index.subsets.semantic_test.items.pairs.file, header=0)\\\n",
    "    .groupby(['type', 'dataset'], as_index=False).size()\n",
    "\n",
    "\n",
    "scores = sLM_21.SLM21ScoreDir(\n",
    "    location=Path('sLM-scores'),\n",
    "    semantic_size=dict(dev=dev_size, test=test_size)\n",
    ")\n",
    "\n",
    "with con.status(\"Building leaderboard...\", spinner=\"aesthetic\"):\n",
    "    led = scores.build_leaderboard()\n",
    "\n",
    "with scores.leaderboard_file.open('w') as fp:\n",
    "    fp.write(led.json(indent=4))\n",
    "\n",
    "print(\"Leaderboard generated successfully\")\n",
    "# d = dict(led._iter(to_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: APP_DIR=/data/zerospeech/zrbench\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaderboard generated successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Testing loading of scores dir of an ABX-LS submission and exporting into leaderboard \"\"\"\n",
    "%env APP_DIR=/data/zerospeech/zrbench\n",
    "from zerospeech.benchmarks import abx_LS\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from rich.console import Console\n",
    "from rich import pretty\n",
    "pretty.install()\n",
    "\n",
    "\n",
    "con = Console()\n",
    "\n",
    "scores = abx_LS.ABXLSScoreDir(\n",
    "    location=Path(\"abxLS-scores\"),\n",
    ")\n",
    "\n",
    "with con.status(\"Building leaderboard...\", spinner=\"aesthetic\"):\n",
    "    led = scores.build_leaderboard()\n",
    "\n",
    "with scores.leaderboard_file.open('w') as fp:\n",
    "    fp.write(led.json(indent=4))\n",
    "\n",
    "print(\"Leaderboard generated successfully\")\n",
    "# d = dict(led._iter(to_dict=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
